{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testy modeli LLM z Ollama\n",
    "W tym notebooku testujemy różne modele LLM oraz różne warianty promptów systemowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista systemowych promptów do testowania\n",
    "system_prompts = [\n",
    "    \"Streszcz zawartość strony internetowej. Odpowiedz po polsku.\",\n",
    "    \"Streszcz zawartość strony w formacie markdown. Użyj punktów, odpowiedz po polsku.\",\n",
    "    \"Podsumuj stronę szczegółowo w języku polskim w formacie markdown.\",\n",
    "    \"Zrób krótkie podsumowanie strony w języku polskim. Format: markdown.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele do testowania\n",
    "models = ['llama3', 'mistral', 'gemma']\n",
    "test_url = 'https://pl.wikipedia.org/wiki/Sztuczna_inteligencja'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(content, system_prompt):\n",
    "    return [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': f'Streszcz tę stronę: {content[:4000]}'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_url(url, model, system_prompt):\n",
    "    content = get_page_content(url)\n",
    "    messages = create_messages(content, system_prompt)\n",
    "    response = ollama.chat(model=model, messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary(summary):\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowy test – model llama3, pierwszy system prompt\n",
    "summary = summarize_url(test_url, model='llama3', system_prompt=system_prompts[0])\n",
    "show_summary(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
